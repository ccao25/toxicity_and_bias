!pip install datasets transformers evaluate -q
!pip install torch
!pip install xformers
!pip install openai
# !pip install torch

# In[2]:


import json
import requests

# In[4]:


import openai
import os

openai.api_key = "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"


# In[5]:


import random
import evaluate
from datasets import load_dataset

toxicity_prompts = load_dataset("allenai/real-toxicity-prompts", split="train")


# In[6]:


print(toxicity_prompts[10]['prompt'])


# In[7]:


def get_random_prompts(dataset, num_examples=100):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    return(dataset[picks])


# In[8]:


toxic_sample= get_random_prompts(toxicity_prompts)
toxic_prompts = [p['text'] for p in toxic_sample['prompt']]

for i in toxic_prompts:
    print(i)

# In[10]:


# Generate text using the GPT-3.5-turbo model

    
def get_completion(prompt, model="gpt-3.5-turbo"):
    
    messages = [{"role": "user", "content": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
        max_tokens=50
    )
    return response.choices[0].message["content"]


# In[11]:


import time 


# In[12]:


model_continuations=[]
for prompt in toxic_prompts:
    continuation = get_completion(prompt)
    print(continuation)
    model_continuations.append(continuation)
    time.sleep(30)


print('Generated '+ str(len(model_continuations))+ ' continuations')

for i in model_continuations:
    print(i)


# In[13]:


print(toxic_prompts[1:3])


# In[14]:


print(model_continuations[1:3])


# In[15]:


print(toxic_prompts[7])
print(model_continuations[7])


# In[ ]:





# In[16]:


toxicity = evaluate.load("toxicity")


# In[17]:


toxicity_ratio = toxicity.compute(predictions=model_continuations, aggregation="ratio")
print(toxicity_ratio)


# In[18]:


max_toxicity = toxicity.compute(predictions=model_continuations, aggregation="maximum")
print(max_toxicity)


# In[19]:


tox_dict= {}
all_toxicity = toxicity.compute(predictions=model_continuations)
for text, score in zip(model_continuations, all_toxicity['toxicity']):
  tox_dict[text] = score


# In[20]:


tox_dict = (dict(sorted(tox_dict.items(), key=lambda item: item[1], reverse=True)))
print(tox_dict)


# In[26]:


tox_dict= {}
all_toxicity = toxicity.compute(predictions=model_continuations)
for text, score in zip(model_continuations, all_toxicity['toxicity']):
  tox_dict[text] = score

tox_dict = (dict(sorted(tox_dict.items(), key=lambda item: item[1], reverse=True)))
print(tox_dict)


# In[27]:


toxic_val = all_toxicity['toxicity']
print(toxic_val)

print("\n")

print(toxic_prompts[10])
print(model_continuations[10])
print(toxic_val[10])


# In[37]:


import csv


# Specify the file path where you want to save the CSV
file_path = 'toxic_maingpt.csv'

# Open the file in write mode with newline='' to avoid extra line breaks
with open(file_path, 'w', newline='',encoding='utf-8') as csvfile:
        
    writer = csv.writer(csvfile)
    writer.writerow(['Toxic Prompt', 'Continuation', 'Toxicity Value'])  # Write header row
    
    for i in range(100):
        writer.writerow([toxic_prompts[i], model_continuations[i], toxic_val[i]])  # Write data rows

    writer.writerow([toxicity_ratio, max_toxicity])  # Write data rows

print(f"CSV file '{file_path}' has been created successfully.")


